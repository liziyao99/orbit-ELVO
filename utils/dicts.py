import numpy as np
import torch
import typing, math
    
def init_transDict(length:int, state_dim:int, obs_dim:int, action_dim:int, 
                   items:typing.Tuple[str]=("td_targets", "regrets", "advantares"),
                   other_terms:typing.Dict[str,typing.Tuple[int]]={}):
    '''
        dict keys: "states", "obss", "actions", "next_states", "next_obss", "rewards", "dones" and items and other_terms.
        Items are float32 of shape (length,).
        Other_terms are of shape (length,*other_terms[key]).
    '''
    trans_dict = {
        "states": np.zeros((length,state_dim), dtype=np.float32),
        "obss": np.zeros((length,obs_dim), dtype=np.float32),
        "actions": np.zeros((length,action_dim), dtype=np.float32),
        "next_states": np.zeros((length,state_dim), dtype=np.float32),
        "next_obss": np.zeros((length,obs_dim), dtype=np.float32),
        "rewards": np.zeros(length, dtype=np.float32),
        "dones": np.zeros(length, dtype=np.bool_),
    }
    for item in items:
        trans_dict[item] = np.zeros(length, dtype=np.float32)
    for key in other_terms.keys():
        trans_dict[key] = np.zeros((length, *other_terms[key]), dtype=np.float32)
    return trans_dict

def init_transDictBatch(length:int, batch_size:int, state_dim:int, obs_dim:int, action_dim:int,
                   items:typing.Tuple[str]=(),
                   other_terms:typing.Dict[str,typing.Tuple[int]]={}, struct="numpy", device="cpu"):
    '''
        dict keys: "states", "obss", "actions", "next_states", "next_obss", "rewards", "dones" and items and other_terms.
        Items are float32 of shape (length,batch_size).
        Other_terms are of shape (length,batch_size,*other_terms[key]).
    '''
    if struct not in ["numpy", "torch"]:
        raise(ValueError("struct must be \"numpy\" or \"torch\"." ))
    trans_dict = {
        "states": np.zeros((length,batch_size,state_dim), dtype=np.float32),
        "obss": np.zeros((length,batch_size,obs_dim), dtype=np.float32),
        "actions": np.zeros((length,batch_size,action_dim), dtype=np.float32),
        "next_states": np.zeros((length,batch_size,state_dim), dtype=np.float32),
        "next_obss": np.zeros((length,batch_size,obs_dim), dtype=np.float32),
        "rewards": np.zeros((length,batch_size), dtype=np.float32),
        "dones": np.zeros((length,batch_size), dtype=np.bool_),
    }
    for item in items:
        trans_dict[item] = np.zeros((length,batch_size), dtype=np.float32)
    for key in other_terms.keys():
        trans_dict[key] = np.zeros((length,batch_size,*other_terms[key]), dtype=np.float32)

    if struct=="torch":
        for key in trans_dict.keys():
            trans_dict[key] = torch.from_numpy(trans_dict[key]).to(device=device)
    return trans_dict

def concat_dicts(dicts:typing.List[dict]):
    d = {}
    for key in dicts[0].keys():
        if isinstance(dicts[0][key], torch.Tensor):
            d[key] = torch.cat([d[key] for d in dicts], dim=0)
        elif isinstance(dicts[0][key], np.ndarray):
            d[key] = np.concatenate([d[key] for d in dicts], axis=0)
        else:
            d[key] = []
            for i in range(0,len(dicts)):
                d[key].extend(dicts[i][key])
    return d

def cut_dict(trans_dict:dict, length:int|None=None):
    '''
        arg:
            `trans_dict`: generated by `init_transDict`
            `length`: cut length. Cut after first done if None.
        return:
            `trans_dict`: cutted.
    '''
    if length is None:
        dones = trans_dict["dones"] if isinstance(trans_dict["dones"], torch.Tensor) else torch.tensor(trans_dict["dones"]).flatten()
        if not dones.any():
            return trans_dict
        length = dones.nonzero()[0].item()+1
    for key in trans_dict.keys():
        trans_dict[key] = trans_dict[key][:length]
    return trans_dict

def split_dict(trans_dict:dict, batch_size:int|None, shuffle=False):
    dicts = []
    key = list(trans_dict.keys())[0]
    total = len(trans_dict[key])
    if shuffle:
        pass # TODO: shuffle for list
    if batch_size is not None:
        n_batches = math.ceil(total/batch_size)
        for i in range(n_batches):
            d = {}
            for key in trans_dict.keys():
                d[key] = trans_dict[key][i*batch_size:(i+1)*batch_size]
            dicts.append(d)
    else:
        dicts = [trans_dict]
    return dicts

def deBatch_dict(trans_dict:dict) -> typing.List[dict]:
    '''
        arg:
            `trans_dict`: generated by `init_transDictBatch`
        return:
            `dicts`: each entry in `trans_dict`
    '''
    dicts = []
    for i in range(trans_dict["states"].shape[1]):
        d = {}
        for key in trans_dict.keys():
            d[key] = trans_dict[key][:,i]
        dicts.append(d)
    return dicts

def numpy_dict(trans_dict:dict) -> dict:
    for key in trans_dict.keys():
        if isinstance(trans_dict[key], torch.Tensor):
            trans_dict[key] = trans_dict[key].detach().cpu().numpy()
        elif isinstance(trans_dict[key], typing.Iterable) and isinstance(trans_dict[key][0], torch.Tensor):
            trans_dict[key] = [t.detach().cpu().numpy() for t in trans_dict[key]]
    return trans_dict

def torch_dict(trans_dict:dict, device=None, detach=False) -> dict:
    for key in trans_dict.keys():
        if type(trans_dict[key]) is np.ndarray:
            trans_dict[key] = torch.from_numpy(trans_dict[key]).float()
        elif isinstance(trans_dict[key], typing.Iterable) and isinstance(trans_dict[key][0], np.ndarray):
            if detach:
                trans_dict[key] = [torch.from_numpy(t).detach().float().to(device) for t in trans_dict[key]]
            else:
                trans_dict[key] = [torch.from_numpy(t).float().to(device) for t in trans_dict[key]]
        if isinstance(trans_dict[key], torch.Tensor):
            trans_dict[key] = trans_dict[key].to(device=device)
            if detach:
                trans_dict[key] = trans_dict[key].detach()
    return trans_dict

def safeStack_dict(trans_dict:dict,
                   no_stack_keys = ["debris_states", "next_debris_states", "debris_obss", "next_debris_obss"]) -> dict:
    for key in trans_dict.keys():
        if key in no_stack_keys:
            continue
        elif isinstance(trans_dict[key], torch.Tensor|np.ndarray):
            continue
        elif isinstance(trans_dict[key], typing.Iterable) and isinstance(trans_dict[key][0], np.ndarray):
            trans_dict[key] = np.stack(trans_dict[key])
        elif isinstance(trans_dict[key], typing.Iterable) and isinstance(trans_dict[key][0], torch.Tensor):
            trans_dict[key] = torch.stack(trans_dict[key])
        elif isinstance(trans_dict[key], typing.Iterable) and isinstance(trans_dict[key][0], np.ScalarType):
            trans_dict[key] = np.stack(trans_dict[key])
    return trans_dict
